{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "380376b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import*\n",
    "#from my_CNN_model import*\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51efa23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(test=False):\n",
    "    \"\"\"\n",
    "    Loads data from FTEST if *test* is True, otherwise from FTRAIN.\n",
    "    Important that the files are in a `data` directory\n",
    "    \"\"\"\n",
    "    FTRAIN = 'D:\\\\Gloify\\\\4-1-2022\\\\New folder\\\\training.csv'\n",
    "    #FTEST = 'data/test.csv'\n",
    "    fname = FTEST if test else FTRAIN\n",
    "    df = pd.read_csv(os.path.expanduser(fname))  # load dataframes\n",
    "\n",
    "    # The Image column has pixel values separated by space; convert\n",
    "    # the values to numpy arrays:\n",
    "    df['Image'] = df['Image'].apply(lambda im: np.fromstring(im, sep=' '))\n",
    "\n",
    "    df = df.dropna()  # drop all rows that have missing values in them\n",
    "\n",
    "    X = np.vstack(df['Image'].values) / 255.  # scale pixel values to [0, 1] (Normalizing)\n",
    "    X = X.astype(np.float32)\n",
    "    X = X.reshape(-1, 96, 96, 1) # return each images as 96 x 96 x 1\n",
    "\n",
    "    if not test:  # only FTRAIN has target columns\n",
    "        y = df[df.columns[:-1]].values\n",
    "        y = (y - 48) / 48  # scale target coordinates to [-1, 1] (Normalizing)\n",
    "        X, y = shuffle(X, y, random_state=42)  # shuffle train data\n",
    "        y = y.astype(np.float32)\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6911472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout\n",
    "from keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "\n",
    "def get_my_CNN_model_architecture():\n",
    "    '''\n",
    "    The network should accept a 96x96 grayscale image as input, and it should output a vector with 30 entries,\n",
    "    corresponding to the predicted (horizontal and vertical) locations of 15 facial keypoints.\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, (5, 5), input_shape=(96,96,1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Convolution2D(30, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(30))\n",
    "\n",
    "    return model;\n",
    "\n",
    "def compile_my_CNN_model(model, optimizer, loss, metrics):\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "def train_my_CNN_model(model, X_train, y_train):\n",
    "    return model.fit(X_train, y_train, epochs=100, batch_size=200, verbose=1, validation_split=0.2)\n",
    "\n",
    "def save_my_CNN_model(model, fileName):\n",
    "    model.save(fileName + '.h5')\n",
    "\n",
    "def load_my_CNN_model(fileName):\n",
    "    return load_model(fileName + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "437cb7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 73s 3s/step - loss: 0.1247 - accuracy: 0.4477 - val_loss: 0.0374 - val_accuracy: 0.6963\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 20s 2s/step - loss: 0.0257 - accuracy: 0.5319 - val_loss: 0.0206 - val_accuracy: 0.6963\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 20s 2s/step - loss: 0.0107 - accuracy: 0.6871 - val_loss: 0.0145 - val_accuracy: 0.6963\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0072 - accuracy: 0.7160 - val_loss: 0.0155 - val_accuracy: 0.6963\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0065 - accuracy: 0.7176 - val_loss: 0.0062 - val_accuracy: 0.6963\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 20s 2s/step - loss: 0.0062 - accuracy: 0.7048 - val_loss: 0.0099 - val_accuracy: 0.6963\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 20s 2s/step - loss: 0.0051 - accuracy: 0.7027 - val_loss: 0.0082 - val_accuracy: 0.6963\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 22s 2s/step - loss: 0.0049 - accuracy: 0.7126 - val_loss: 0.0080 - val_accuracy: 0.6963\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 24s 3s/step - loss: 0.0047 - accuracy: 0.6935 - val_loss: 0.0089 - val_accuracy: 0.6963\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 28s 3s/step - loss: 0.0049 - accuracy: 0.7009 - val_loss: 0.0088 - val_accuracy: 0.6963\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0048 - accuracy: 0.6994 - val_loss: 0.0079 - val_accuracy: 0.6963\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 30s 3s/step - loss: 0.0046 - accuracy: 0.6980 - val_loss: 0.0076 - val_accuracy: 0.6963\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 26s 3s/step - loss: 0.0045 - accuracy: 0.7127 - val_loss: 0.0068 - val_accuracy: 0.6963\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 28s 3s/step - loss: 0.0046 - accuracy: 0.7036 - val_loss: 0.0070 - val_accuracy: 0.6963\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 23s 3s/step - loss: 0.0045 - accuracy: 0.7156 - val_loss: 0.0075 - val_accuracy: 0.6963\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0046 - accuracy: 0.6908 - val_loss: 0.0061 - val_accuracy: 0.6963\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0045 - accuracy: 0.7016 - val_loss: 0.0057 - val_accuracy: 0.6963\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0046 - accuracy: 0.7008 - val_loss: 0.0060 - val_accuracy: 0.6963\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0045 - accuracy: 0.7151 - val_loss: 0.0071 - val_accuracy: 0.6963\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.0043 - accuracy: 0.7171 - val_loss: 0.0063 - val_accuracy: 0.6963\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 30s 3s/step - loss: 0.0045 - accuracy: 0.7134 - val_loss: 0.0060 - val_accuracy: 0.6963\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 28s 3s/step - loss: 0.0043 - accuracy: 0.7177 - val_loss: 0.0050 - val_accuracy: 0.6963\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 30s 3s/step - loss: 0.0044 - accuracy: 0.7156 - val_loss: 0.0048 - val_accuracy: 0.6963\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 38s 4s/step - loss: 0.0043 - accuracy: 0.7038 - val_loss: 0.0050 - val_accuracy: 0.6963\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 26s 3s/step - loss: 0.0043 - accuracy: 0.6936 - val_loss: 0.0050 - val_accuracy: 0.6963\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 24s 3s/step - loss: 0.0043 - accuracy: 0.7215 - val_loss: 0.0054 - val_accuracy: 0.6963\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 26s 3s/step - loss: 0.0043 - accuracy: 0.7083 - val_loss: 0.0047 - val_accuracy: 0.6963\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 22s 2s/step - loss: 0.0042 - accuracy: 0.7141 - val_loss: 0.0045 - val_accuracy: 0.6963\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 24s 3s/step - loss: 0.0041 - accuracy: 0.7112 - val_loss: 0.0049 - val_accuracy: 0.6963\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 24s 3s/step - loss: 0.0040 - accuracy: 0.7018 - val_loss: 0.0046 - val_accuracy: 0.6963\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0041 - accuracy: 0.7109 - val_loss: 0.0053 - val_accuracy: 0.6963\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.0041 - accuracy: 0.7050 - val_loss: 0.0047 - val_accuracy: 0.6963\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 30s 3s/step - loss: 0.0039 - accuracy: 0.7156 - val_loss: 0.0044 - val_accuracy: 0.6963\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0039 - accuracy: 0.7057 - val_loss: 0.0043 - val_accuracy: 0.6963\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 26s 3s/step - loss: 0.0037 - accuracy: 0.6978 - val_loss: 0.0041 - val_accuracy: 0.6939\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 24s 3s/step - loss: 0.0035 - accuracy: 0.7064 - val_loss: 0.0038 - val_accuracy: 0.6963\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 23s 3s/step - loss: 0.0035 - accuracy: 0.7139 - val_loss: 0.0037 - val_accuracy: 0.6986\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0034 - accuracy: 0.7003 - val_loss: 0.0034 - val_accuracy: 0.6963\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0032 - accuracy: 0.7051 - val_loss: 0.0031 - val_accuracy: 0.6963\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0031 - accuracy: 0.7105 - val_loss: 0.0034 - val_accuracy: 0.6963\n",
      "Epoch 41/100\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0030 - accuracy: 0.7091 - val_loss: 0.0029 - val_accuracy: 0.6963\n",
      "Epoch 42/100\n",
      "9/9 [==============================] - 22s 2s/step - loss: 0.0029 - accuracy: 0.7108 - val_loss: 0.0028 - val_accuracy: 0.6963\n",
      "Epoch 43/100\n",
      "9/9 [==============================] - 23s 3s/step - loss: 0.0028 - accuracy: 0.7264 - val_loss: 0.0028 - val_accuracy: 0.6963\n",
      "Epoch 44/100\n",
      "9/9 [==============================] - 24s 3s/step - loss: 0.0028 - accuracy: 0.6905 - val_loss: 0.0030 - val_accuracy: 0.6963\n",
      "Epoch 45/100\n",
      "9/9 [==============================] - 25s 3s/step - loss: 0.0027 - accuracy: 0.6987 - val_loss: 0.0030 - val_accuracy: 0.6963\n",
      "Epoch 46/100\n",
      "9/9 [==============================] - 26s 3s/step - loss: 0.0026 - accuracy: 0.7058 - val_loss: 0.0026 - val_accuracy: 0.6986\n",
      "Epoch 47/100\n",
      "9/9 [==============================] - 22s 2s/step - loss: 0.0026 - accuracy: 0.7113 - val_loss: 0.0025 - val_accuracy: 0.6986\n",
      "Epoch 48/100\n",
      "9/9 [==============================] - 25s 3s/step - loss: 0.0025 - accuracy: 0.7101 - val_loss: 0.0025 - val_accuracy: 0.6986\n",
      "Epoch 49/100\n",
      "9/9 [==============================] - 24s 3s/step - loss: 0.0025 - accuracy: 0.7171 - val_loss: 0.0024 - val_accuracy: 0.6963\n",
      "Epoch 50/100\n",
      "9/9 [==============================] - 23s 3s/step - loss: 0.0024 - accuracy: 0.7129 - val_loss: 0.0025 - val_accuracy: 0.6939\n",
      "Epoch 51/100\n",
      "9/9 [==============================] - 25s 3s/step - loss: 0.0024 - accuracy: 0.7153 - val_loss: 0.0026 - val_accuracy: 0.7033\n",
      "Epoch 52/100\n",
      "9/9 [==============================] - 23s 3s/step - loss: 0.0023 - accuracy: 0.7153 - val_loss: 0.0025 - val_accuracy: 0.7009\n",
      "Epoch 53/100\n",
      "9/9 [==============================] - 22s 2s/step - loss: 0.0023 - accuracy: 0.7132 - val_loss: 0.0025 - val_accuracy: 0.7126\n",
      "Epoch 54/100\n",
      "9/9 [==============================] - 26s 3s/step - loss: 0.0022 - accuracy: 0.7260 - val_loss: 0.0022 - val_accuracy: 0.7126\n",
      "Epoch 55/100\n",
      "9/9 [==============================] - 22s 2s/step - loss: 0.0022 - accuracy: 0.7201 - val_loss: 0.0020 - val_accuracy: 0.7056\n",
      "Epoch 56/100\n",
      "9/9 [==============================] - 22s 2s/step - loss: 0.0021 - accuracy: 0.7178 - val_loss: 0.0021 - val_accuracy: 0.7150\n",
      "Epoch 57/100\n",
      "9/9 [==============================] - 23s 3s/step - loss: 0.0021 - accuracy: 0.7190 - val_loss: 0.0019 - val_accuracy: 0.7103\n",
      "Epoch 58/100\n",
      "9/9 [==============================] - 24s 3s/step - loss: 0.0021 - accuracy: 0.7263 - val_loss: 0.0019 - val_accuracy: 0.7173\n",
      "Epoch 59/100\n",
      "9/9 [==============================] - 24s 3s/step - loss: 0.0020 - accuracy: 0.7434 - val_loss: 0.0020 - val_accuracy: 0.7220\n",
      "Epoch 60/100\n",
      "9/9 [==============================] - 25s 3s/step - loss: 0.0020 - accuracy: 0.7372 - val_loss: 0.0017 - val_accuracy: 0.7243\n",
      "Epoch 61/100\n",
      "9/9 [==============================] - 26s 3s/step - loss: 0.0020 - accuracy: 0.7305 - val_loss: 0.0019 - val_accuracy: 0.7243\n",
      "Epoch 62/100\n",
      "9/9 [==============================] - 23s 3s/step - loss: 0.0019 - accuracy: 0.7169 - val_loss: 0.0017 - val_accuracy: 0.7266\n",
      "Epoch 63/100\n",
      "9/9 [==============================] - 22s 2s/step - loss: 0.0019 - accuracy: 0.7151 - val_loss: 0.0018 - val_accuracy: 0.7196\n",
      "Epoch 64/100\n",
      "9/9 [==============================] - 23s 3s/step - loss: 0.0018 - accuracy: 0.7459 - val_loss: 0.0018 - val_accuracy: 0.7173\n",
      "Epoch 65/100\n",
      "9/9 [==============================] - 22s 2s/step - loss: 0.0018 - accuracy: 0.7209 - val_loss: 0.0017 - val_accuracy: 0.7173\n",
      "Epoch 66/100\n",
      "9/9 [==============================] - 27s 3s/step - loss: 0.0018 - accuracy: 0.7366 - val_loss: 0.0018 - val_accuracy: 0.7336\n",
      "Epoch 67/100\n",
      "9/9 [==============================] - 26s 3s/step - loss: 0.0017 - accuracy: 0.7370 - val_loss: 0.0018 - val_accuracy: 0.7220\n",
      "Epoch 68/100\n",
      "9/9 [==============================] - 22s 2s/step - loss: 0.0017 - accuracy: 0.7274 - val_loss: 0.0018 - val_accuracy: 0.7196\n",
      "Epoch 69/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0017 - accuracy: 0.7433 - val_loss: 0.0018 - val_accuracy: 0.7220\n",
      "Epoch 70/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0016 - accuracy: 0.7144 - val_loss: 0.0017 - val_accuracy: 0.7336\n",
      "Epoch 71/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0016 - accuracy: 0.7290 - val_loss: 0.0016 - val_accuracy: 0.7196\n",
      "Epoch 72/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0016 - accuracy: 0.7297 - val_loss: 0.0016 - val_accuracy: 0.7313\n",
      "Epoch 73/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0016 - accuracy: 0.7447 - val_loss: 0.0014 - val_accuracy: 0.7266\n",
      "Epoch 74/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0016 - accuracy: 0.7407 - val_loss: 0.0015 - val_accuracy: 0.7383\n",
      "Epoch 75/100\n",
      "9/9 [==============================] - 20s 2s/step - loss: 0.0016 - accuracy: 0.7467 - val_loss: 0.0015 - val_accuracy: 0.7383\n",
      "Epoch 76/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0016 - accuracy: 0.7519 - val_loss: 0.0014 - val_accuracy: 0.7477\n",
      "Epoch 77/100\n",
      "9/9 [==============================] - 22s 2s/step - loss: 0.0014 - accuracy: 0.7467 - val_loss: 0.0015 - val_accuracy: 0.7477\n",
      "Epoch 78/100\n",
      "9/9 [==============================] - 22s 2s/step - loss: 0.0015 - accuracy: 0.7468 - val_loss: 0.0014 - val_accuracy: 0.7453\n",
      "Epoch 79/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0015 - accuracy: 0.7396 - val_loss: 0.0013 - val_accuracy: 0.7313\n",
      "Epoch 80/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0015 - accuracy: 0.7451 - val_loss: 0.0014 - val_accuracy: 0.7640\n",
      "Epoch 81/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0014 - accuracy: 0.7597 - val_loss: 0.0013 - val_accuracy: 0.7547\n",
      "Epoch 82/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0014 - accuracy: 0.7468 - val_loss: 0.0014 - val_accuracy: 0.7453\n",
      "Epoch 83/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0013 - accuracy: 0.7541 - val_loss: 0.0013 - val_accuracy: 0.7523\n",
      "Epoch 84/100\n",
      "9/9 [==============================] - 22s 2s/step - loss: 0.0014 - accuracy: 0.7438 - val_loss: 0.0013 - val_accuracy: 0.7593\n",
      "Epoch 85/100\n",
      "9/9 [==============================] - 23s 3s/step - loss: 0.0014 - accuracy: 0.7699 - val_loss: 0.0013 - val_accuracy: 0.7360\n",
      "Epoch 86/100\n",
      "9/9 [==============================] - 22s 2s/step - loss: 0.0014 - accuracy: 0.7599 - val_loss: 0.0014 - val_accuracy: 0.7453\n",
      "Epoch 87/100\n",
      "9/9 [==============================] - 20s 2s/step - loss: 0.0014 - accuracy: 0.7570 - val_loss: 0.0013 - val_accuracy: 0.7687\n",
      "Epoch 88/100\n",
      "9/9 [==============================] - 20s 2s/step - loss: 0.0013 - accuracy: 0.7458 - val_loss: 0.0013 - val_accuracy: 0.7640\n",
      "Epoch 89/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0013 - accuracy: 0.7644 - val_loss: 0.0013 - val_accuracy: 0.7710\n",
      "Epoch 90/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0013 - accuracy: 0.7656 - val_loss: 0.0013 - val_accuracy: 0.7500\n",
      "Epoch 91/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0013 - accuracy: 0.7507 - val_loss: 0.0013 - val_accuracy: 0.7640\n",
      "Epoch 92/100\n",
      "9/9 [==============================] - 20s 2s/step - loss: 0.0013 - accuracy: 0.7474 - val_loss: 0.0012 - val_accuracy: 0.7617\n",
      "Epoch 93/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0013 - accuracy: 0.7729 - val_loss: 0.0013 - val_accuracy: 0.7640\n",
      "Epoch 94/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0012 - accuracy: 0.7766 - val_loss: 0.0012 - val_accuracy: 0.7617\n",
      "Epoch 95/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0012 - accuracy: 0.7775 - val_loss: 0.0012 - val_accuracy: 0.7734\n",
      "Epoch 96/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0012 - accuracy: 0.7768 - val_loss: 0.0012 - val_accuracy: 0.7430\n",
      "Epoch 97/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0012 - accuracy: 0.7611 - val_loss: 0.0012 - val_accuracy: 0.7500\n",
      "Epoch 98/100\n",
      "9/9 [==============================] - 23s 3s/step - loss: 0.0012 - accuracy: 0.7739 - val_loss: 0.0012 - val_accuracy: 0.7500\n",
      "Epoch 99/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0012 - accuracy: 0.7585 - val_loss: 0.0012 - val_accuracy: 0.7617\n",
      "Epoch 100/100\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0011 - accuracy: 0.7753 - val_loss: 0.0012 - val_accuracy: 0.7734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# You can skip all the steps above (from 'Setting the CNN architecture') after running the script for the first time.\\n# Just load the recent model using load_my_CNN_model and use it to predict keypoints on any face data\\nmy_model = load_my_CNN_model('my_model')\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load training set\n",
    "X_train, y_train = load_data()\n",
    "\n",
    "# NOTE: Please check the load_data() method in utils.py to see how the data is preprocessed (normalizations and stuff)\n",
    "\n",
    "\n",
    "# Setting the CNN architecture\n",
    "my_model = get_my_CNN_model_architecture()\n",
    "\n",
    "# Compiling the CNN model with an appropriate optimizer and loss and metrics\n",
    "compile_my_CNN_model(my_model, optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "hist = train_my_CNN_model(my_model, X_train, y_train)\n",
    "\n",
    "# train_my_CNN_model returns a History object. History.history attribute is a record of training loss values and metrics\n",
    "# values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n",
    "\n",
    "# Saving the model\n",
    "save_my_CNN_model(my_model, 'my_model')\n",
    "\n",
    "'''\n",
    "# You can skip all the steps above (from 'Setting the CNN architecture') after running the script for the first time.\n",
    "# Just load the recent model using load_my_CNN_model and use it to predict keypoints on any face data\n",
    "my_model = load_my_CNN_model('my_model')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dceb497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the model built in the previous step\n",
    "my_model = load_my_CNN_model('my_model')\n",
    "\n",
    "# Face cascade to detect faces\n",
    "face_cascade = cv2.CascadeClassifier('D:\\\\Gloify\\\\4-1-2022\\\\New folder\\\\snapchat-like-shades-filter-master\\\\cascades\\\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Define the upper and lower boundaries for a color to be considered \"Blue\"\n",
    "blueLower = np.array([100, 60, 60])\n",
    "blueUpper = np.array([140, 255, 255])\n",
    "\n",
    "# Define a 5x5 kernel for erosion and dilation\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "# Define filters\n",
    "filters = ['D:\\\\Gloify\\\\4-1-2022\\\\New folder\\\\snapchat-like-shades-filter-master\\\\images\\\\sunglasses.png']\n",
    "filterIndex = 0\n",
    "\n",
    "# Load the video\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "# Keep looping\n",
    "while True:\n",
    "    # Grab the current paintWindow\n",
    "    (grabbed, frame) = camera.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame2 = np.copy(frame)\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.25, 6)\n",
    "\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "\n",
    "        # Grab the face\n",
    "        gray_face = gray[y:y+h, x:x+w]\n",
    "        color_face = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Normalize to match the input format of the model - Range of pixel to [0, 1]\n",
    "        gray_normalized = gray_face / 255\n",
    "\n",
    "        # Resize it to 96x96 to match the input format of the model\n",
    "        original_shape = gray_face.shape # A Copy for future reference\n",
    "        face_resized = cv2.resize(gray_normalized, (96, 96), interpolation = cv2.INTER_AREA)\n",
    "        face_resized_copy = face_resized.copy()\n",
    "        face_resized = face_resized.reshape(1, 96, 96, 1)\n",
    "\n",
    "        # Predicting the keypoints using the model\n",
    "        keypoints = my_model.predict(face_resized)\n",
    "\n",
    "        # De-Normalize the keypoints values\n",
    "        keypoints = keypoints * 48 + 48\n",
    "\n",
    "        # Map the Keypoints back to the original image\n",
    "        face_resized_color = cv2.resize(color_face, (96, 96), interpolation = cv2.INTER_AREA)\n",
    "        face_resized_color2 = np.copy(face_resized_color)\n",
    "\n",
    "        # Pair them together\n",
    "        points = []\n",
    "        for i, co in enumerate(keypoints[0][0::2]):\n",
    "            points.append((co, keypoints[0][1::2][i]))\n",
    "\n",
    "        # Add FILTER to the frame\n",
    "        sunglasses = cv2.imread(filters[filterIndex], cv2.IMREAD_UNCHANGED)\n",
    "        sunglass_width = int((points[7][0]-points[9][0])*1.1)\n",
    "        sunglass_height = int((points[10][1]-points[8][1])/1.1)\n",
    "        sunglass_resized = cv2.resize(sunglasses, (sunglass_width, sunglass_height), interpolation = cv2.INTER_CUBIC)\n",
    "        transparent_region = sunglass_resized[:,:,:3] != 0\n",
    "        face_resized_color[int(points[9][1]):int(points[9][1])+sunglass_height, int(points[9][0]):int(points[9][0])+sunglass_width,:][transparent_region] = sunglass_resized[:,:,:3][transparent_region]\n",
    "\n",
    "        # Resize the face_resized_color image back to its original shape\n",
    "        frame[y:y+h, x:x+w] = cv2.resize(face_resized_color, original_shape, interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "        # Add KEYPOINTS to the frame2\n",
    "        for keypoint in points:\n",
    "            #print(keypoint)\n",
    "            cv2.circle(face_resized_color2, (int(keypoint[0]), int(keypoint[1])), 1, (0,255,0), 1)\n",
    "            \n",
    "\n",
    "        frame2[y:y+h, x:x+w] = cv2.resize(face_resized_color2, original_shape, interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "        # Show the frame\n",
    "        cv2.imshow(\"Selfie Filters\", frame)\n",
    "\n",
    "    # If the 'q' key is pressed, stop the loop\n",
    "    key=cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "# Cleanup the camera and close any open windows\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64312560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
